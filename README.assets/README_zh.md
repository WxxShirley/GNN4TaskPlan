# Can Graph Learning Improve Planning in LLM-based Agents?

[![Paper](https://img.shields.io/badge/Paper-arXiv%20Link-blue)](https://arxiv.org/abs/2405.19119)

è¿™æ˜¯æˆ‘ä»¬åˆšè¢«NeurIPS'2024å½•ç”¨çš„è®ºæ–‡çš„å®˜æ–¹ä»£ç å®ç°ã€‚

###  


![task](task.jpg)

ä¸åŒäºå·²æœ‰çš„æç¤ºå·¥ç¨‹å’Œå¤§æ¨¡å‹å¾®è°ƒæ–¹æ³•ï¼Œæˆ‘ä»¬ç ”ç©¶äº†æ­£äº¤äºè¿™ä¸¤ç§ä¸»æµæ–¹æ³•çš„æ–°æ–¹å‘ï¼š**åˆ©ç”¨å›¾å­¦ä¹ æŠ€æœ¯æå‡å¤§æ¨¡å‹æ™ºèƒ½ä½“çš„è§„åˆ’èƒ½åŠ›**ã€‚
åœ¨ä»»åŠ¡è§„åˆ’ä¸­ï¼Œä¸åŒçš„å­ä»»åŠ¡ä¹‹é—´å­˜åœ¨ç€ä¾èµ–å…³ç³»ï¼Œè¿™äº›å…³ç³»å¯ä»¥ç”¨ä»»åŠ¡å›¾ï¼ˆTask Graphï¼‰æ¥è¡¨ç¤ºã€‚æ¯ä¸ªèŠ‚ç‚¹ä»£è¡¨ä¸€ä¸ªå­ä»»åŠ¡ï¼Œè¾¹åˆ™è¡¨ç¤ºå­ä»»åŠ¡ä¹‹é—´çš„ä¾èµ–ã€‚
åœ¨ä»»åŠ¡å›¾æ„å»ºå®Œæˆï¼Œä»»åŠ¡è§„åˆ’å¯ä»¥å¾ˆè‡ªç„¶åœ°è½¬åŒ–ä¸º: åœ¨Task Graphä¸­é€‰æ‹©ä¸€æ¡è¿é€šçš„è·¯å¾„æˆ–è€…å­å›¾ï¼Œæ¥æ»¡è¶³ç”¨æˆ·çš„è¯·æ±‚ã€‚
åŸºäºæ­¤ï¼Œæˆ‘ä»¬çš„ç ”ç©¶ä¸»è¦åšå‡ºäº†ä¸‰ç‚¹è´¡çŒ®ï¼š
* ä»»åŠ¡è§„åˆ’çš„å›¾å»ºæ¨¡: æˆ‘ä»¬ç¬¬ä¸€æ¬¡æŠŠä»»åŠ¡è§„åˆ’é—®é¢˜è½¬åŒ–ä¸ºå›¾ä¸Šçš„å†³ç­–é—®é¢˜ï¼Œæ¢ç´¢äº†å›¾å­¦ä¹ æŠ€æœ¯åœ¨è¿™ä¸€é¢†åŸŸçš„æ½œåŠ›ã€‚åŒæ—¶ï¼Œè¿™ä¹Ÿä¸ºå›¾å­¦ä¹ å¸¦æ¥äº†ä¸€ä¸ªæ–°çš„åº”ç”¨åœºæ™¯â€”â€”ä»»åŠ¡è§„åˆ’ã€‚
* ç†è®ºç ”ç©¶: æˆ‘ä»¬æ·±å…¥åˆ†æäº†LLMsåœ¨å¤„ç†å›¾ä»»åŠ¡æ—¶çš„å±€é™æ€§ï¼Œå‘ç°è™½ç„¶Transformerså…·å¤‡ä¸€å®šçš„è¡¨è¾¾èƒ½åŠ›ï¼Œä½†è‡ªæ³¨æ„åŠ›æœºåˆ¶çš„åå·®å’Œè‡ªå›å½’æŸå¤±å¯èƒ½é™åˆ¶äº†å®ƒä»¬åœ¨å›¾å†³ç­–ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚
* ç®—æ³•ä¸å®éªŒ: æˆ‘ä»¬å¼•å…¥äº†GNNæ¥å¢å¼ºLLMsçš„ä»»åŠ¡è§„åˆ’èƒ½åŠ›ï¼Œå®ƒå…·æœ‰Training-freeå’ŒTraining-requiredä¸¤ç§å˜ä½“ã€‚ç»“æœè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸ä»…æ€§èƒ½ä¼˜äºç°æœ‰çš„è§£å†³æ–¹æ¡ˆï¼Œè€Œä¸”è®¡ç®—æ•ˆç‡æ›´é«˜ã€‚

> å¼•ç”¨æ ¼å¼å‚è€ƒ 
```
@article{wu2024graph,
  title={Can Graph Learning Improve Task Planning?},
  author={Xixi Wu and Yifei Shen and Caihua Shan and Kaitao Song and Siwei Wang and Bohang Zhang and Jiarui Feng and Hong Cheng and Wei Chen and Yun Xiong and Dongsheng Li},
  journal={arXiv preprint arXiv:2405.19119},
  year={2024}
}
```


## ç¯å¢ƒé…ç½®

è¿è¡Œæœ¬ä»“åº“æ‰€éœ€è¦çš„ç¯å¢ƒåœ¨`requirements.txt`ä¸­å·²æœ‰å£°æ˜ï¼Œå¯ä»¥é€šè¿‡è¿è¡Œä¸‹é¢çš„æŒ‡ä»¤å®‰è£…æ‰€éœ€çš„ç¯å¢ƒ:

```shell
pip install -r requirements.txt
```

### éƒ¨ç½²å¼€æºçš„å¤§æ¨¡å‹

å¯¹äºTraining-freeçš„æ–¹æ³•ï¼Œå³LLMç›´æ¥æ¨ç†(Direct Inference)å’ŒLLMè¿›è¡Œå›¾æœç´¢(GraphSearch)ï¼Œæˆ‘ä»¬çš„ä»£ç å®ç°æ˜¯åˆ©ç”¨`FastChat`(https://github.com/lm-sys/FastChat) æŠŠLLMéƒ¨ç½²æˆæœ¬åœ°å¯è°ƒç”¨çš„APIæœåŠ¡å®ç°çš„ï¼Œå…·ä½“æ¥è¯´éƒ¨ç½²åœ¨äº†æœ¬åœ°çš„`8008`ç«¯å£ã€‚

* å®‰è£…FastChat  
  ```shell
  pip3 install "fschat[model_worker,webui]"
  pip3 install vllm
  ```

* å®‰è£…å®Œæˆåï¼Œé€šè¿‡ä¾æ¬¡æ‰§è¡Œä¸‹é¢çš„ä¸‰ä¸ªæŒ‡ä»¤æ¥éƒ¨ç½²æŸä¸ªæŒ‡å®šçš„å¤§æ¨¡å‹
  ```shell
  # é¦–å…ˆéœ€è¦å¯åŠ¨Controller
  python3 -m fastchat.serve.controller --host 127.0.0.1 

  # ç„¶åæ˜¯æŒ‡æ˜éœ€è¦éƒ¨ç½²çš„LLMåç§°ï¼Œå½“ç¬¬ä¸€æ¬¡æ‰§è¡Œè¿™ä¸ªæŒ‡ä»¤çš„æ—¶å€™ï¼Œä¼šè‡ªåŠ¨ä¸‹è½½è¿™ä¸ªLLMçš„Checkpoints
  python3 -m fastchat.serve.vllm_worker --model-path codellama/CodeLlama-13b-Instruct-hf --host 127.0.0.1
  # éƒ¨ç½²å…¶ä»–LLMçš„ç¤ºä¾‹ï¼Œåªè¦æ›´æ¢`model-path`å°±å¯ä»¥äº†
  # python3 -m fastchat.serve.vllm_worker --model-path lmsys/vicuna-13b-v1.5 --host 127.0.0.1
  # python3 -m fastchat.serve.vllm_worker --model-path codellama/CodeLlama-7b-Instruct-hf --host 127.0.0.1
  
  # æœ€åæ˜¯å¼€å¯æœ¬åœ°çš„APIæœåŠ¡ï¼Œéœ€è¦æŒ‡æ˜å¯¹åº”çš„ç«¯å£
  python3 -m fastchat.serve.openai_api_server --host 127.0.0.1 --port 8008
  ```
  
  æ•…éšœæ’æŸ¥
  <details show=hide>
  
  1. å¦‚æœå¯åŠ¨æœåŠ¡åä»æ— æ³•æ”¶åˆ°è¯·æ±‚LLMçš„å“åº”ï¼Œå¯ä»¥åœ¨Controllerçš„æŒ‡ä»¤ä¸­åˆ‡æ¢hostï¼Œå¦‚ä¸‹æ‰€ç¤º
     ```shell
     python3 -m fastchat.serve.controller --host 0.0.0.0 
     ```
  2. å¦‚æœå¯åŠ¨`vllm_worker`é‡åˆ°å¦‚ä¸‹çš„æŠ¥é”™:
       ```log
       2024-07-22 17:30:50 | ERROR | stderr |   File "/root/miniconda3/lib/python3.8/site-packages/vllm/engine/async_llm_engine.py", line 228, in _run_workers_async
       2024-07-22 17:30:50 | ERROR | stderr |     assert output == other_output
       2024-07-22 17:30:50 | ERROR | stderr | AssertionError
       ```
     å¯ä»¥ (i) è¿›å…¥åˆ°`site-packages/vllm/engine/async_llm_engine.py`æ–‡ä»¶ä¸­ï¼Œå°†ä¸Šè¿°æ–­è¨€çš„éƒ¨åˆ†æ³¨é‡Šæ‰(227-228è¡Œ)ï¼Œæˆ–è€…
     (ii) ç”¨`model_worker`ä»£æ›¿`vllm_worker`ï¼Œå³
     ```shell 
     # model-pathæˆ‘å¡«å†™äº†å°†LLMå­˜å‚¨åœ¨æœ¬åœ°çš„ä¸€ä¸ªè·¯å¾„ï¼Œè¯·æ ¹æ®å®é™…å¡«å†™
     python3 -m fastchat.serve.model_worker --model-path /root/autodl-tmp/models/AI-ModelScope/Mistral-7B-Instruct-v0.2 --host 127.0.0.1 

     python3 -m fastchat.serve.openai_api_server --host 127.0.0.1  --port 8008 
     ```
  </details>


## ä»“åº“å†…å®¹é¢„è§ˆ 

```
.
â”œâ”€â”€ GraphToken                     --> å¯¹æ¯”æ¨¡å‹GraphTokençš„å¤ç°
â”œâ”€â”€ README.assets    
â”œâ”€â”€ README.md       
â”œâ”€â”€ data                           --> æä¾›äº†äº”ä¸ªå®éªŒçš„æ•°æ®é›†å’Œå¤„ç†çš„ä»£ç 
â”‚   â”œâ”€â”€ dailylife
â”‚   â”œâ”€â”€ huggingface
â”‚   â”œâ”€â”€ multimedia
â”‚   â”œâ”€â”€ raw                        --> RestBenchå’ŒUltraToolçš„åŸå§‹æ•°æ®
â”‚   â”‚   â””â”€â”€ RestBench                    `https://github.com/Yifan-Song793/RestGPT`
â”‚Â Â  â”‚Â Â  â””â”€â”€ ultratool                    `https://github.com/JoeYing1019/UltraTool`
â”‚   â”œâ”€â”€ raw_process_restgpt.py     --> å¤„ç†RestBenchçš„ä»£ç 
â”‚   â”œâ”€â”€ raw_process_ultratool.py   --> å¤„ç†UltraToolçš„ä»£ç 
â”‚   â”œâ”€â”€ split_data.py              --> æµ‹è¯•é›†åˆ’åˆ†
â”‚Â Â  â”œâ”€â”€ tmdb
â”‚Â Â  â””â”€â”€ ultratool
â”œâ”€â”€ evaluate.py                    --> éªŒè¯çš„ä»£ç 
â”œâ”€â”€ finetunellm                    --> LLMå¾®è°ƒå’Œå¾®è°ƒåçš„LLMæ¨ç†çš„ä»£ç 
â”œâ”€â”€ finetunellm_script.sh          --> LLMå¾®è°ƒæŒ‡ä»¤
â”œâ”€â”€ prediction                     --> LLMç›´æ¥æ¨ç†çš„å†…å®¹
â”œâ”€â”€ requirements.txt     
â”œâ”€â”€ trainfree                      --> Training-freeæ–¹æ³•çš„ä»£ç (Direct, GraphSearch, and SGC)
â”œâ”€â”€ trainfree_script.sh            --> Training-freeæ–¹æ³•çš„è¿è¡ŒæŒ‡ä»¤
â”œâ”€â”€ traingnn                       --> Training-requiredæ–¹æ³•çš„ä»£ç 
â”œâ”€â”€ traingnn_reproduce.sh          --> Training-requiredæ–¹æ³•çš„è¿è¡Œå’Œå¤ç°çš„æŒ‡ä»¤
â””â”€â”€ utils                 
```

è¿™ä¸ªä»“åº“é‡Œï¼Œæˆ‘ä»¬åŒæ—¶æä¾›äº†Baselineçš„å®ç°ã€æ‰€æœ‰å®éªŒæ•°æ®é›†çš„æ•°æ®å’Œå¤„ç†ä»£ç ã€Training-freeå’ŒTraining-requiredä»£ç ã€ä»¥åŠLLMåœ¨è§„åˆ’ä»»åŠ¡ä¸Šå¾®è°ƒçš„ä»£ç ï¼Œåé¢æˆ‘ä»¬ä¼šé€ä¸€ä»‹ç»æ¯ä¸€éƒ¨åˆ†ã€‚


## æ•°æ®é›†

æ‰€æœ‰æ•°æ®é›†çš„å†…å®¹å’Œå¤„ç†è¿‡ç¨‹éƒ½åœ¨`data`æ–‡ä»¶å¤¹ä¸‹ã€‚
æˆ‘ä»¬ä¸€å…±ä½¿ç”¨äº†äº”ä¸ªæ•°æ®é›†ï¼Œå…¶ä¸­HuggingFaceï¼ŒMultimediaï¼ŒDailyLifeæ¥è‡ªäº[TaskBench](https://github.com/microsoft/JARVIS/blob/main/taskbench/README.md)ï¼ŒTMDBæ¥è‡ªäº[RestBench](https://github.com/Yifan-Song793/RestGPT)ï¼Œå¦å¤–è¡¥å……äº†[UltraTool](https://github.com/JoeYing1019/UltraTool)ã€‚

æ¯ä¸ªæ•°æ®é›†éƒ½ç”±ä»¥ä¸‹æ–‡ä»¶æ„æˆï¼š
* `data.json` æ•°æ®é›†çš„å…·ä½“ä¿¡æ¯ï¼Œæ¯ä¸€æ¡æ ·æœ¬ç”±ç”¨æˆ·è¯·æ±‚ã€å¯¹åº”åˆ†è§£çš„æ­¥éª¤ã€å’Œä»»åŠ¡è°ƒç”¨é“¾æ„æˆ
* `graph_desc.json` è¯¥æ•°æ®é›†çš„ä»»åŠ¡å›¾ï¼ŒåŒ…æ‹¬èŠ‚ç‚¹å’Œè¾¹
* `tool_desc.json` è¯¥æ•°æ®é›†çš„æ‰€æœ‰ä»»åŠ¡ï¼Œå³ä¸Šä¸€ä¸ªæ–‡ä»¶ä¸­çš„èŠ‚ç‚¹ä¿¡æ¯
* `user_requests.json` æ•°æ®é›†ä¸­çš„ç”¨æˆ·è¯·æ±‚
* `split_ids.json` åˆ’åˆ†çš„è®­ç»ƒé›†ID


TaskBenchæˆ‘ä»¬ä½¿ç”¨äº†è¯¥Benchmarkå¤„ç†å¥½çš„æ•°æ®é›†ã€‚

RestBenchçš„å¤„ç†è¿‡ç¨‹åœ¨`data/raw_process_restgpt.py`ä¸­ï¼ŒåŸå§‹çš„æ•°æ®åœ¨`data/raw/RestBench`ä¸­ã€‚ç”±äºRestBenchä¸­æ¯ä¸ªä»»åŠ¡(ä¸€ä¸ªAPI)åªæœ‰APIå¯¹åº”çš„è¯·æ±‚åœ°å€ï¼Œæˆ‘ä»¬é¦–å…ˆèµ‹äºˆäº†æ¯ä¸ªAPIä¸€ä¸ªç‹¬ç‰¹çš„åç§°å’Œå…·ä½“çš„åŠŸèƒ½æè¿°ã€‚è¿™ä¹‹åï¼ŒåŸºäºä»»åŠ¡ä¹‹é—´çš„å‚æ•°ä¾èµ–å’Œç±»å‹æ„å»ºäº†ä»»åŠ¡å›¾ã€‚æœ€åï¼Œå¯¹åŸå§‹çš„æ•°æ®è¿›è¡Œé‡æ„æˆä¸Šè¿°çš„æ ‡å‡†æ ¼å¼ï¼Œå¹¶æç¤ºGPT-4æ¥ç”Ÿæˆæ¯ä¸ªè¯·æ±‚å¯¹åº”çš„åˆ†è§£æ­¥éª¤ï¼Œæ°å¥½å’Œè°ƒç”¨çš„ä»»åŠ¡å¯¹é½ã€‚

ä¸ºäº†æ¼”ç¤ºåœ¨å¤§çš„ä»»åŠ¡å›¾ä¸Šçš„æ‹“å±•æ€§ï¼Œæˆ‘ä»¬å¼•å…¥äº†æ–°çš„æ•°æ®é›†UltraToolï¼Œå¤„ç†è¿‡ç¨‹åœ¨`data/raw_process_ultratool.py`ä¸­ï¼ŒåŸå§‹çš„æ•°æ®åœ¨`data/raw/ultratool`ä¸­ã€‚ç”±äºåŸå§‹çš„UltraToolæ•°æ®é›†è§„æ¨¡è¾ƒå¤§ã€ä¸”å¾ˆå¤šTaskåªå‡ºç°åœ¨1-2ä¸ªæ ·æœ¬ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆå¯¹æ•°æ®é›†è¿›è¡Œäº†è¿‡æ»¤ï¼šåªè€ƒè™‘è°ƒç”¨çš„ä»»åŠ¡æ•°>=2çš„æ ·æœ¬ã€ä¸”åªè€ƒè™‘å‡ºç°çš„é¢‘ç‡>=5çš„ä»»åŠ¡ã€‚æˆ‘ä»¬åŸºäºè¿‡æ»¤åçš„æ ·æœ¬å’Œä»»åŠ¡æ„å»ºäº†ä»»åŠ¡å›¾ï¼Œå¹¶ç”¨ç›¸ä¼¼çš„æ–¹æ³•æç¤ºGPTç”Ÿæˆæ­¥éª¤æè¿°ï¼Œå®Œæˆäº†æ•°æ®é›†çš„é‡æ„ã€‚


## Training-freeçš„ä»£ç 
```
â”œâ”€â”€ trainfree
â”‚Â Â  â”œâ”€â”€ direct.py               --> LLMç›´æ¥æ¨ç†çš„ä»£ç (Direct Inference)
â”‚Â Â  â”œâ”€â”€ direct_diffprompt.py    --> LLMç›´æ¥æ¨ç†ä½¿ç”¨äº†ä¸åŒæç¤ºæ¨¡ç‰ˆçš„ä»£ç ï¼ŒåŒ…æ‹¬(i)æ›´å¤šçš„ä¸Šä¸‹æ–‡å­¦ä¹ ä¾‹å­å’Œ(ii)Plan like a Graph
â”‚Â Â  â”œâ”€â”€ graphsearch.py          --> LLMè¿›è¡Œå›¾æœç´¢çš„ä»£ç (GraphSearch)
â”‚Â Â  â””â”€â”€ sgc.py                  --> æˆ‘ä»¬çš„SGC
```

Training-freeçš„æ–¹æ³•æœ‰ï¼š

* LLMç›´æ¥æ¨ç† **Direct Inference**  æˆ‘ä»¬é»˜è®¤ä½¿ç”¨äº†1-shotä¸Šä¸‹æ–‡å­¦ä¹ çš„è®¾ç½®ï¼Œä½¿ç”¨çš„æç¤ºå‚è€ƒäº†[TaskBench](https://github.com/microsoft/JARVIS/blob/main/taskbench/inference.py)
  > åœ¨`direct_diffprompt.py`ä¸­ï¼Œæˆ‘ä»¬æä¾›äº†é¢å¤–çš„æç¤ºæ¨¡ç‰ˆï¼Œç›¸æ¯”äºé»˜è®¤çš„1-shotèƒ½å®ç°æ›´å¥½çš„ç»“æœã€‚å…¶ä¸­Plan like a Graph(PlaG)æ˜¯å—åˆ°è¯¥è®ºæ–‡["Graph-enhanced Large Language Models in Asynchronous Plan Reasoning"](https://arxiv.org/abs/2402.02805)çš„å¯å‘ã€‚

* LLMè¿›è¡Œå›¾æœç´¢ **GraphSearch**  LLMåœ¨ä»»åŠ¡å›¾ä¸Šè¿›è¡Œå¯å‘å¼æœç´¢ï¼Œåœ¨å¤šæ¡éå†åˆ°çš„è·¯å¾„ä¸­é€‰æ‹©ä¸€æ¡æœ€åˆé€‚çš„ä½œä¸ºè§„åˆ’ç»“æœã€‚æ ¹æ®æœç´¢ç­–ç•¥çš„ä¸åŒï¼Œæœ‰BeamSearch, GreedySearch, å’ŒAdaptiveSearchè¿™ä¸‰ä¸ªå˜ä½“ã€‚æˆ‘ä»¬çš„å®ç°å‚è€ƒäº†[ControlLLM](https://github.com/OpenGVLab/ControlLLM/blob/main/cllm/services/tog/tool.py).
* **SGC**  æˆ‘ä»¬æå‡ºçš„æ— éœ€è®­ç»ƒçš„GNNæ–¹æ³•ï¼Œä½¿ç”¨äº†ä¸€ä¸ªæ²¡æœ‰å‚æ•°çš„GNNæ¨¡å‹ï¼Œå³SGCï¼Œè¿›è¡Œä»»åŠ¡çš„æ£€ç´¢æ¥å®Œæˆè§„åˆ’



æ‰€æœ‰è¿è¡Œçš„æŒ‡ä»¤å’Œå‚æ•°è®¾ç½®åœ¨è„šæœ¬`trainfree_script.sh`ä¸­ã€‚

âš ï¸ **æ³¨æ„**ï¼šåœ¨è¿è¡ŒGraphSearch/SGCä¹‹å‰ï¼Œéœ€è¦å…ˆæœ‰è¯¥LLMä¸‹çš„ç›´æ¥æ¨ç†æ–‡ä»¶è·å¾—çš„æ­¥éª¤ä¿¡æ¯ã€‚



## Training-required GNNä»£ç 
```
â”œâ”€â”€ traingnn
â”‚   â”œâ”€â”€ gnn.py              --> ä¸åŒGNNç¼–ç å™¨çš„å®ç°ï¼ŒåŒ…æ‹¬SGC, GCN, GAT, SAGE, GIN, and TransformerConv
â”‚   â”œâ”€â”€ main.py             --> å¯åŠ¨æ–‡ä»¶
â”‚   â”œâ”€â”€ model.py            --> LM+GNNæ¨¡å‹
â”‚   â””â”€â”€ sampler.py          --> é‡‡æ ·å™¨ï¼Œå½¢æˆ `<æ­¥éª¤ï¼Œæ­£æ ·æœ¬ä»»åŠ¡ï¼Œè´Ÿæ ·æœ¬ä»»åŠ¡>`ä¸‰å…ƒç»„æ¥è®­ç»ƒæ¨¡å‹
â”œâ”€â”€ traingnn_reproduce.sh   --> å¤ç°æ‰€æœ‰å®éªŒç»“æœçš„æŒ‡ä»¤
```

è¿è¡Œ`main.py`æ–‡ä»¶å¯ä»¥å®Œæˆ(LM+)GNNçš„è®­ç»ƒå’Œæµ‹è¯•ï¼Œç‰¹åˆ«åœ°ï¼Œæˆ‘ä»¬è§£é‡Šä»¥ä¸‹çš„å‚æ•°ä½¿ç”¨:
* `lm_name`: LMç¼–ç å™¨çš„åç§°ï¼Œé»˜è®¤ä¸º`intfloat/e5-large`
* `gnn_name`: GNNç¼–ç å™¨çš„åç§°
* `gnn_hidden_dim`: GNNçš„éšè—å±‚ç»´åº¦
* `num_negatives`: æ¯ä¸ªæ­£æ ·æœ¬æ‰€é‡‡é›†çš„è´Ÿæ ·æœ¬æ•°é‡ï¼Œé»˜è®¤ä¸º`2`
* `text_negative`: è´Ÿæ ·æœ¬æ˜¯å¦å’Œæ­£æ ·æœ¬ç¼–ç è¡¨å¾æ¥è¿‘
* `lm_frozen`: LMçš„å‚æ•°æ˜¯å¦å†»ç»“ï¼Œå¦‚æœæ˜¯`1`ï¼Œåˆ™LMå†»ç»“ã€åªè®­ç»ƒGNNï¼›å¦åˆ™`0`ï¼Œæ„å‘³ç€LMä¸GNNå…±åŒè®­ç»ƒ

è¿è¡Œçš„æŒ‡ä»¤ç¤ºä¾‹ï¼Œæ›´å¤šåœ°åœ¨`traingnn_reproduce.sh`ä¸­ã€‚
```shell
# HuggingFaceæ•°æ®é›†ï¼Œåªè®­ç»ƒGNN
python main.py --lm_frozen=1 --epoch=10 --text_negative=1 --gnn_name=SAGE --lr=0.001

# HuggingFaceæ•°æ®é›†ï¼ŒLM+GNNå…±åŒè®­ç»ƒ
python main.py --lm_frozen=0 --epoch=20 --text_negative=1 --gnn_name=SAGE
```



## LLMså¾®è°ƒ
```
â”œâ”€â”€ finetunellm
â”‚Â Â  â”œâ”€â”€ inference.py         --> Direct inference of fine-tuned LLMs
â”‚Â Â  â”œâ”€â”€ main.py              --> Fine-tuning LLM
â”‚Â Â  â””â”€â”€ user_prompt.py       --> Instruction Template
```

æˆ‘ä»¬åœ¨è®ºæ–‡ä¸­æ¢ç©¶äº†æ–¹æ³•å¯¹äºå¾®è°ƒè¿‡çš„LLM(å³è·å¾—äº†æŒ‡å®šæ•°æ®é›†ä¸Šçš„è§„åˆ’èƒ½åŠ›)çš„æœ‰æ•ˆæ€§ï¼Œå…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å¯¹[CodeLlama-7B](https://huggingface.co/codellama/CodeLlama-7b-Instruct-hf)å’Œ[Vicuna-13B](https://huggingface.co/lmsys/vicuna-13b-v1.5), ä½¿ç”¨LoRAæŠ€æœ¯è¿›è¡Œäº†å¾®è°ƒã€‚

`main.py`æ–‡ä»¶ä¸­æä¾›äº†LLMå¾®è°ƒçš„ä»£ç ï¼Œ`inference.py`æ˜¯æ¨¡å‹å¾®è°ƒå®Œæˆåè¿›è¡Œæ¨ç†çš„ä»£ç (è¿è¡Œæ—¶éœ€è¦æŒ‡å®šLLMçš„åç§°å’Œcheckpointçš„è·¯å¾„)ã€‚

æ‰€æœ‰çš„è¿è¡ŒæŒ‡ä»¤åœ¨`finetunellm_script.sh`ä¸­ï¼Œåœ¨å®éªŒä¸­æˆ‘ä»¬ä½¿ç”¨äº†2å¼ A100è¿›è¡Œå¾®è°ƒï¼Œå¹¶åœ¨è¯¥è®¾å¤‡æ¡ä»¶ä¸‹è¿›è¡Œçš„è®­ç»ƒå‚æ•°é…ç½®ã€‚è¯»è€…å¯ä»¥æ ¹æ®è®¾å¤‡æ¡ä»¶ä¿®æ”¹è®­ç»ƒçš„å‚æ•°ã€‚



## ç»“æœéªŒè¯

`evaluate.py`æä¾›äº†å¯¹è§„åˆ’ç»“æœçš„ç»Ÿä¸€éªŒè¯è¿‡ç¨‹ï¼Œè¯„ä¼°çš„æŒ‡æ ‡åŒ…æ‹¬: `Node-F1`, `Link-F1`, `Node-Hallucination`, `Link-Hallucination`.

ä¸ºäº†æ–¹ä¾¿è¯»è€…å¤ç°ï¼Œæˆ‘ä»¬å·²ç»åœ¨`prediction`æ–‡ä»¶å¤¹ä¸‹æä¾›äº†ä¸¤ä¸ªLLMsåœ¨HuggingFaceæ•°æ®é›†ä¸Šç›´æ¥æ¨ç†çš„ç»“æœï¼Œå¯ä»¥é€šè¿‡è¿è¡Œå¦‚ä¸‹æŒ‡ä»¤ï¼ŒæŸ¥çœ‹ä¸åŒLLMç›´æ¥æ¨ç†ç»“æœçš„å¾—åˆ†ï¼Œå…¶ä¸­`llm`ä¸ºè¦è¯„ä¼°çš„LLMçš„åç§° `method`ä¸ºå¯¹åº”çš„æ–¹æ³•åç§°ï¼š

```shell 
python evaluate.py --llm=CodeLlama-13b --dataset=huggingface --method=direct
```

å¯¹åº”çš„ç»“æœå¦‚ä¸‹ï¼Œ`NF`æŒ‡Node-F1å¾—åˆ†ï¼Œ`LF`æŒ‡Link-F1å¾—åˆ†ï¼Œ`NH`æŒ‡èŠ‚ç‚¹çš„å¹»è§‰ç‡ï¼ˆ1/2åˆ†åˆ«ä»£è¡¨å¾®è§‚å’Œå®è§‚ï¼‰ï¼Œ`LH`æŒ‡è¾¹çš„å¹»è§‰ç‡
```
+-------------+---------------+-------+--------+--------+--------+--------+--------+--------+
|   Dataset   |      LLM      |  Mode |   NF   |   LF   |  NH-1  |  NH-2  |  LH-1  |  LH-2  |
+-------------+---------------+-------+--------+--------+--------+--------+--------+--------+
| huggingface | CodeLlama-13b | chain | 0.5755 | 0.2888 | 0.1656 | 0.4306 | 0.4228 | 0.6338 |
+-------------+---------------+-------+--------+--------+--------+--------+--------+--------+
```


## Baselineå¤ç°

æˆ‘ä»¬æä¾›äº†å¯¹Training-requiredæ–¹æ³•ï¼ŒGraphTokenï¼Œåœ¨è§„åˆ’ä»»åŠ¡ä¸Šçš„å¤ç°ä»£ç ã€‚

è¿è¡Œçš„æŒ‡ä»¤åœ¨`GraphToken/run.sh`ä¸­ï¼Œè¯»è€…å¯ä»¥æ ¹æ®è‡ªèº«ä½¿ç”¨çš„å®éªŒè®¾å¤‡è°ƒæ•´è®­ç»ƒå‚æ•°ã€‚

> Perozzi, Bryan, et al. "Let Your Graph Do the Talking: Encoding Structured Data for LLMs." arXiv preprint, 2024.


---

ğŸ“® å¦‚æœåœ¨è¿è¡Œæ—¶è¿˜é‡åˆ°å…¶ä»–é—®é¢˜ï¼Œæ¬¢è¿åœ¨IssueåŒºå±•å¼€è®¨è®ºæˆ–é‚®ä»¶è”ç³»ä½œè€…: xxwu@se.cuhk.edu.hk 

